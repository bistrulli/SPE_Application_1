{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3: M/M/1 Model Validation\n",
    "## Theoretical Predictions vs Real System Measurements\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Estimate service rate from real system measurements\n",
    "- Validate M/M/1 theoretical formulas against measured metrics\n",
    "- Analyze model accuracy under varying load conditions\n",
    "- Understand practical deviations from theoretical predictions\n",
    "\n",
    "---\n",
    "\n",
    "## Methodology\n",
    "\n",
    "### Service Rate Estimation Strategy\n",
    "\n",
    "Instead of estimating service rate from individual response times, we use a more robust approach:\n",
    "\n",
    "1. **Calibration Campaign**: Run controlled experiment with known load\n",
    "2. **Collect Metrics**: Measure utilization and throughput via Prometheus\n",
    "3. **Calculate Service Rate**: μ = throughput / utilization\n",
    "4. **Validation Experiments**: Test multiple load conditions using estimated μ\n",
    "\n",
    "This approach provides more accurate service rate estimation by using steady-state system metrics rather than individual request timings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module 3: M/M/1 Model Validation\n",
      "Modules imported successfully\n",
      "Ready for service rate calibration and validation experiments\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import asyncio\n",
    "from scipy import stats\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import our custom modules\n",
    "from workload_generator import AsyncWorkloadGenerator\n",
    "from metrics_collector import PrometheusCollector, plot_mm1_validation_analysis, calculate_validation_statistics\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Module 3: M/M/1 Model Validation\")\n",
    "print(\"Modules imported successfully\")\n",
    "print(\"Ready for service rate calibration and validation experiments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M/M/1 Theoretical Calculator with improved service rate estimation ready\n"
     ]
    }
   ],
   "source": [
    "class MM1Theoretical:\n",
    "    \"\"\"\n",
    "    M/M/1 theoretical performance calculations with improved service rate estimation.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_metrics(lambda_rate: float, mu_rate: float) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Calculate all M/M/1 theoretical metrics.\n",
    "        \n",
    "        Args:\n",
    "            lambda_rate: Arrival rate (requests/second)\n",
    "            mu_rate: Service rate (requests/second)\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with theoretical metrics\n",
    "        \"\"\"\n",
    "        if lambda_rate >= mu_rate:\n",
    "            return {\n",
    "                'utilization': float('inf'),\n",
    "                'response_time': float('inf'),\n",
    "                'system_size': float('inf'),\n",
    "                'queue_length': float('inf'),\n",
    "                'waiting_time': float('inf'),\n",
    "                'stable': False\n",
    "            }\n",
    "        \n",
    "        rho = lambda_rate / mu_rate\n",
    "        \n",
    "        return {\n",
    "            'utilization': rho,\n",
    "            'response_time': 1 / (mu_rate - lambda_rate),\n",
    "            'system_size': rho / (1 - rho),\n",
    "            'queue_length': (rho ** 2) / (1 - rho),\n",
    "            'waiting_time': rho / (mu_rate * (1 - rho)),\n",
    "            'throughput': lambda_rate,  # In stable system, throughput = arrival rate\n",
    "            'stable': True\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def estimate_service_rate_from_metrics(utilizations: List[float], throughputs: List[float]) -> Tuple[float, Dict]:\n",
    "        \"\"\"\n",
    "        Estimate service rate from measured utilization and throughput metrics.\n",
    "        \n",
    "        Theory: utilization = λ/μ, so μ = λ/utilization ≈ throughput/utilization\n",
    "        \n",
    "        Args:\n",
    "            utilizations: List of measured utilization values (0-1)\n",
    "            throughputs: List of measured throughput values (req/s)\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (estimated_mu, estimation_stats)\n",
    "        \"\"\"\n",
    "        if not utilizations or not throughputs:\n",
    "            return None, {'error': 'No data provided'}\n",
    "            \n",
    "        # Filter out invalid measurements\n",
    "        valid_pairs = [(util, tp) for util, tp in zip(utilizations, throughputs) \n",
    "                      if util > 0.01 and tp > 0.01]  # Avoid division by very small numbers\n",
    "        \n",
    "        if len(valid_pairs) < 5:\n",
    "            return None, {'error': f'Insufficient valid measurements: {len(valid_pairs)}'}\n",
    "        \n",
    "        # Calculate service rate for each measurement: μ = throughput / utilization\n",
    "        service_rates = []\n",
    "        for util, throughput in valid_pairs:\n",
    "            mu = throughput / util\n",
    "            service_rates.append(mu)\n",
    "        \n",
    "        # Calculate statistics\n",
    "        estimated_mu = np.median(service_rates)\n",
    "        mean_mu = np.mean(service_rates)\n",
    "        std_mu = np.std(service_rates)\n",
    "        cv_mu = std_mu / mean_mu if mean_mu > 0 else float('inf')\n",
    "        \n",
    "        estimation_stats = {\n",
    "            'sample_size': len(service_rates),\n",
    "            'median_mu': estimated_mu,\n",
    "            'mean_mu': mean_mu,\n",
    "            'std_mu': std_mu,\n",
    "            'cv_mu': cv_mu,\n",
    "            'min_mu': np.min(service_rates),\n",
    "            'max_mu': np.max(service_rates),\n",
    "            'valid_measurements': len(valid_pairs),\n",
    "            'total_measurements': len(utilizations)\n",
    "        }\n",
    "        \n",
    "        return estimated_mu, estimation_stats\n",
    "\n",
    "print(\"M/M/1 Theoretical Calculator with improved service rate estimation ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Parameters\n",
    "\n",
    "Configure all experiment parameters in one centralized location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment Configuration:\n",
      "  Calibration: λ=3.0 req/s, duration=360.0s\n",
      "  Validation: 6 utilization levels, 360s each\n",
      "  Total estimated time: 42.0 minutes\n",
      "Configuration complete\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# EXPERIMENT CONFIGURATION PARAMETERS\n",
    "# =============================================================================\n",
    "\n",
    "# Calibration experiment parameters\n",
    "CALIBRATION_LAMBDA = 3.0      # Conservative arrival rate for calibration\n",
    "CALIBRATION_DURATION = 360.0  # Duration of calibration experiment (seconds)\n",
    "CALIBRATION_WARMUP = 60.0     # Warmup period to reach steady state\n",
    "\n",
    "# Validation experiment parameters  \n",
    "TARGET_UTILIZATIONS = [0.3, 0.4, 0.5, 0.6, 0.7,0.8]  # 10%, 30%, 50%, 70%, 80%\n",
    "VALIDATION_DURATION = 360     # Duration per validation experiment (seconds)\n",
    "VALIDATION_WARMUP = 60        # Warmup period per validation experiment\n",
    "VALIDATION_COOLDOWN = 60        # COOLDOWN period beween two experiments\n",
    "\n",
    "# Metrics collection parameters\n",
    "METRICS_STEP = \"5s\"           # Prometheus query resolution\n",
    "CORE_METRICS = ['throughput', 'cpu_usage', 'response_time_avg']\n",
    "\n",
    "# System health check\n",
    "PROMETHEUS_URL = \"http://localhost:9090\"\n",
    "\n",
    "print(\"Experiment Configuration:\")\n",
    "print(f\"  Calibration: λ={CALIBRATION_LAMBDA} req/s, duration={CALIBRATION_DURATION}s\")\n",
    "print(f\"  Validation: {len(TARGET_UTILIZATIONS)} utilization levels, {VALIDATION_DURATION}s each\")\n",
    "print(f\"  Total estimated time: {(CALIBRATION_DURATION + len(TARGET_UTILIZATIONS) * VALIDATION_DURATION) / 60:.1f} minutes\")\n",
    "print(\"Configuration complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Service Rate Calibration Campaign\n",
    "\n",
    "Run a controlled experiment to estimate the service rate of our M/M/1 system using Prometheus metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CALIBRATION CAMPAIGN EXECUTION\n",
    "# =============================================================================\n",
    "\n",
    "async def run_calibration_campaign():\n",
    "    \"\"\"\n",
    "    Execute service rate calibration campaign.\n",
    "    \n",
    "    Returns:\n",
    "        estimated_mu: Estimated service rate\n",
    "        estimation_stats: Calibration statistics\n",
    "    \"\"\"\n",
    "    print(\"Service Rate Calibration Campaign\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"Calibration parameters:\")\n",
    "    print(f\"  Target arrival rate: {CALIBRATION_LAMBDA} req/s\")\n",
    "    print(f\"  Experiment duration: {CALIBRATION_DURATION} seconds\")\n",
    "    print(f\"  Warmup period: {CALIBRATION_WARMUP} seconds\")\n",
    "    print(f\"  Measurement period: {CALIBRATION_DURATION - CALIBRATION_WARMUP} seconds\")\n",
    "\n",
    "    # System health check before calibration\n",
    "    collector = PrometheusCollector(PROMETHEUS_URL)\n",
    "    if not collector.health_check():\n",
    "        collector.close()\n",
    "        raise Exception(\"Prometheus not accessible - cannot proceed\")\n",
    "\n",
    "    print(\"\\nPrometheus connectivity confirmed\")\n",
    "    collector.close()\n",
    "\n",
    "    print(\"\\nStarting calibration experiment...\")\n",
    "    print(\"This will take approximately 3 minutes\")\n",
    "    \n",
    "    # Initialize generators and collectors\n",
    "    async_generator = AsyncWorkloadGenerator()\n",
    "    collector = PrometheusCollector(PROMETHEUS_URL)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Generate calibration workload\n",
    "    print(\"\\nRunning calibration workload...\")\n",
    "    calibration_results = await async_generator.generate_workload(\n",
    "        lambda_rate=CALIBRATION_LAMBDA,\n",
    "        duration=CALIBRATION_DURATION,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(\"\\nCollecting calibration metrics...\")\n",
    "    \n",
    "    # Collect metrics for steady-state period (skip warmup)\n",
    "    steady_state_start = start_time + CALIBRATION_WARMUP\n",
    "    calibration_metrics = collector.get_metrics_for_timerange(\n",
    "        start_time=steady_state_start,\n",
    "        end_time=end_time,\n",
    "        metrics=['throughput', 'cpu_usage'],  # Only core metrics needed for calibration\n",
    "        step=METRICS_STEP\n",
    "    )\n",
    "    \n",
    "    collector.close()\n",
    "    \n",
    "    print(f\"\\nCalibration experiment completed:\")\n",
    "    print(f\"  Workload requests: {calibration_results.total_requests}\")\n",
    "    print(f\"  Workload rate: {calibration_results.actual_rate:.2f} req/s\")\n",
    "    print(f\"  Success rate: {calibration_results.success_rate:.1%}\")\n",
    "    \n",
    "    # Process calibration data\n",
    "    print(\"\\nProcessing calibration metrics...\")\n",
    "    \n",
    "    # Extract utilization measurements (CPU query returns fraction 0-1)\n",
    "    utilizations = []\n",
    "    if 'cpu_usage' in calibration_metrics and calibration_metrics['cpu_usage'].points:\n",
    "        utilizations = [p.value for p in calibration_metrics['cpu_usage'].points \n",
    "                       if p.value > 0]  # Direct use, already in fraction form\n",
    "    else:\n",
    "        print(\"Warning: No CPU usage data available\")\n",
    "\n",
    "    # Extract throughput measurements  \n",
    "    throughputs = []\n",
    "    if 'throughput' in calibration_metrics and calibration_metrics['throughput'].points:\n",
    "        throughputs = [p.value for p in calibration_metrics['throughput'].points \n",
    "                      if p.value > 0]\n",
    "    else:\n",
    "        print(\"Warning: No throughput data from Prometheus, using workload generator data\")\n",
    "        # Fallback to workload generator rate\n",
    "        throughputs = [calibration_results.actual_rate] * len(utilizations)\n",
    "\n",
    "    print(f\"Collected {len(utilizations)} utilization measurements\")\n",
    "    print(f\"Collected {len(throughputs)} throughput measurements\")\n",
    "\n",
    "    if utilizations and throughputs:\n",
    "        print(f\"Utilization range: {np.min(utilizations):.3f} - {np.max(utilizations):.3f}\")\n",
    "        print(f\"Throughput range: {np.min(throughputs):.3f} - {np.max(throughputs):.3f} req/s\")\n",
    "        \n",
    "        # Estimate service rate\n",
    "        estimated_mu, estimation_stats = MM1Theoretical.estimate_service_rate_from_metrics(\n",
    "            utilizations, throughputs\n",
    "        )\n",
    "        \n",
    "        if estimated_mu:\n",
    "            print(f\"\\nService Rate Estimation Results:\")\n",
    "            print(f\"  Estimated μ (median): {estimated_mu:.2f} req/s\")\n",
    "            print(f\"  Mean μ: {estimation_stats['mean_mu']:.2f} req/s\")\n",
    "            print(f\"  Standard deviation: {estimation_stats['std_mu']:.2f}\")\n",
    "            print(f\"  Coefficient of variation: {estimation_stats['cv_mu']:.3f}\")\n",
    "            print(f\"  Range: {estimation_stats['min_mu']:.2f} - {estimation_stats['max_mu']:.2f} req/s\")\n",
    "            print(f\"  Valid measurements: {estimation_stats['valid_measurements']}/{estimation_stats['total_measurements']}\")\n",
    "            \n",
    "            # Implied service time\n",
    "            estimated_service_time = 1.0 / estimated_mu\n",
    "            print(f\"  Implied service time: {estimated_service_time:.4f} seconds\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"\\nService rate estimation failed:\")\n",
    "            print(f\"  Error: {estimation_stats.get('error', 'Unknown error')}\")\n",
    "            estimated_mu = 10.0  # Fallback value\n",
    "            estimation_stats = {'error': 'Using fallback value'}\n",
    "            print(f\"  Using fallback μ = {estimated_mu} req/s\")\n",
    "    else:\n",
    "        print(\"\\nInsufficient calibration data - using fallback service rate\")\n",
    "        estimated_mu = 10.0\n",
    "        estimation_stats = {'error': 'Insufficient data'}\n",
    "\n",
    "    print(f\"\\nCalibration phase complete\")\n",
    "    print(f\"Using μ = {estimated_mu:.2f} req/s for validation experiments\")\n",
    "    \n",
    "    return estimated_mu, estimation_stats\n",
    "\n",
    "# Execute calibration campaign\n",
    "estimated_mu, calibration_stats = await run_calibration_campaign()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Experiments\n",
    "\n",
    "Execute systematic validation experiments across multiple utilization levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VALIDATION EXPERIMENTS EXECUTION\n",
    "# =============================================================================\n",
    "\n",
    "async def run_validation_experiments(estimated_mu: float):\n",
    "    \"\"\"\n",
    "    Execute M/M/1 validation experiments across multiple utilization levels.\n",
    "    \n",
    "    Args:\n",
    "        estimated_mu: Estimated service rate from calibration\n",
    "        \n",
    "    Returns:\n",
    "        measured_metrics: Dictionary with measured metrics for each lambda\n",
    "        theoretical_predictions: Dictionary with theoretical metrics for each lambda\n",
    "        validation_results: Complete raw results from experiments\n",
    "    \"\"\"\n",
    "    \n",
    "    # Design validation experiments based on estimated service rate\n",
    "    print(\"Validation Experiment Design\")\n",
    "    print(\"=\" * 35)\n",
    "\n",
    "    # Calculate test conditions\n",
    "    test_lambdas = [estimated_mu * util for util in TARGET_UTILIZATIONS]\n",
    "\n",
    "    print(f\"Estimated service rate (μ): {estimated_mu:.2f} req/s\")\n",
    "    print(f\"Validation test conditions:\")\n",
    "    for i, (util, lambda_rate) in enumerate(zip(TARGET_UTILIZATIONS, test_lambdas), 1):\n",
    "        print(f\"  {i}. Target ρ = {util:.1%}, λ = {lambda_rate:.2f} req/s\")\n",
    "\n",
    "    print(f\"\\nValidation parameters:\")\n",
    "    print(f\"  Duration per experiment: {VALIDATION_DURATION}s\")\n",
    "    print(f\"  Warmup per experiment: {VALIDATION_WARMUP}s\")\n",
    "\n",
    "    total_time = len(test_lambdas) * VALIDATION_DURATION / 60\n",
    "    print(f\"  Estimated total validation time: {total_time:.1f} minutes\")\n",
    "\n",
    "    # Calculate theoretical predictions for all test cases\n",
    "    theoretical_predictions = {}\n",
    "    print(f\"\\nTheoretical predictions:\")\n",
    "    for lambda_rate, util in zip(test_lambdas, TARGET_UTILIZATIONS):\n",
    "        predictions = MM1Theoretical.calculate_metrics(lambda_rate, estimated_mu)\n",
    "        theoretical_predictions[lambda_rate] = predictions\n",
    "\n",
    "        if predictions['stable']:\n",
    "            print(f\"  λ={lambda_rate:.2f}: ρ={predictions['utilization']:.2f}, \"\n",
    "                  f\"E[T]={predictions['response_time']:.3f}s, E[N]={predictions['system_size']:.2f}\")\n",
    "        else:\n",
    "            print(f\"  λ={lambda_rate:.2f}: UNSTABLE SYSTEM\")\n",
    "\n",
    "    print(\"\\nReady to begin validation experiments\")\n",
    "    \n",
    "    # Execute validation experiments\n",
    "    print(\"\\nStarting M/M/1 Validation Experiments\")\n",
    "    print(\"=\" * 40)\n",
    "\n",
    "    validation_results = {}\n",
    "    measured_metrics = {}\n",
    "\n",
    "    for i, lambda_rate in enumerate(test_lambdas, 1):\n",
    "        expected_util = lambda_rate / estimated_mu\n",
    "        print(f\"\\nValidation Experiment {i}/{len(test_lambdas)}:\")\n",
    "        print(f\"  Target: λ = {lambda_rate:.2f} req/s, ρ = {expected_util:.1%}\")\n",
    "\n",
    "        # Run validation experiment\n",
    "        async_generator = AsyncWorkloadGenerator()\n",
    "        collector = PrometheusCollector(PROMETHEUS_URL)\n",
    "\n",
    "        print(f\"  Starting workload generation...\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Generate workload\n",
    "        workload_results = await async_generator.generate_workload(\n",
    "            lambda_rate=lambda_rate,\n",
    "            duration=VALIDATION_DURATION,\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Collect metrics for steady-state period\n",
    "        print(f\"  Collecting system metrics...\")\n",
    "        steady_state_start = start_time + VALIDATION_WARMUP\n",
    "        metrics_data = collector.get_metrics_for_timerange(\n",
    "            start_time=steady_state_start,\n",
    "            end_time=end_time,\n",
    "            metrics=CORE_METRICS,  # Include server-side response time\n",
    "            step=METRICS_STEP\n",
    "        )\n",
    "\n",
    "        collector.close()\n",
    "\n",
    "        # Process measurements - use both client-side and server-side data\n",
    "        measured_throughput = workload_results.actual_rate\n",
    "        measured_response_time_client = workload_results.average_response_time  # Client-side measurement\n",
    "        measured_response_time_server = None  # Server-side measurement from Prometheus\n",
    "        measured_utilization = lambda_rate / estimated_mu  # Fallback calculation\n",
    "\n",
    "        # Extract Prometheus metrics if available\n",
    "        if 'throughput' in metrics_data and metrics_data['throughput'].points:\n",
    "            prometheus_throughput = np.mean([p.value for p in metrics_data['throughput'].points if p.value > 0])\n",
    "            if prometheus_throughput > 0:\n",
    "                measured_throughput = prometheus_throughput\n",
    "\n",
    "        if 'cpu_usage' in metrics_data and metrics_data['cpu_usage'].points:\n",
    "            prometheus_cpu = np.mean([p.value for p in metrics_data['cpu_usage'].points if p.value > 0])\n",
    "            if prometheus_cpu > 0:\n",
    "                # CPU query returns fraction directly (0-1), no conversion needed\n",
    "                measured_utilization = prometheus_cpu\n",
    "\n",
    "        # Extract server-side response time from Prometheus\n",
    "        if 'response_time_avg' in metrics_data and metrics_data['response_time_avg'].points:\n",
    "            prometheus_response_times = [p.value for p in metrics_data['response_time_avg'].points if p.value > 0]\n",
    "            if prometheus_response_times:\n",
    "                measured_response_time_server = np.mean(prometheus_response_times)\n",
    "\n",
    "        # Choose response time measurement: prefer server-side if available, fallback to client-side\n",
    "        if measured_response_time_server is not None:\n",
    "            measured_response_time = measured_response_time_server\n",
    "            response_time_source = \"server-side (Prometheus)\"\n",
    "        else:\n",
    "            measured_response_time = measured_response_time_client\n",
    "            response_time_source = \"client-side\"\n",
    "\n",
    "        # Store results\n",
    "        validation_results[lambda_rate] = {\n",
    "            'workload_results': workload_results,\n",
    "            'metrics_data': metrics_data\n",
    "        }\n",
    "\n",
    "        measured_metrics[lambda_rate] = {\n",
    "            'throughput': measured_throughput,\n",
    "            'response_time': measured_response_time,\n",
    "            'response_time_client': measured_response_time_client,\n",
    "            'response_time_server': measured_response_time_server,\n",
    "            'response_time_source': response_time_source,\n",
    "            'utilization': measured_utilization,\n",
    "            'success_rate': workload_results.success_rate\n",
    "        }\n",
    "\n",
    "        # Print experiment summary\n",
    "        theoretical = theoretical_predictions[lambda_rate]\n",
    "        print(f\"  Results:\")\n",
    "        print(f\"    Throughput: {measured_throughput:.2f} req/s (theory: {theoretical['throughput']:.2f})\")\n",
    "        print(f\"    Response time: {measured_response_time:.3f}s ({response_time_source}, theory: {theoretical['response_time']:.3f})\")\n",
    "        if measured_response_time_client is not None and measured_response_time_server is not None:\n",
    "            diff = abs(measured_response_time_client - measured_response_time_server) / measured_response_time_client * 100\n",
    "            print(f\"      Client vs Server RT diff: {diff:.1f}%\")\n",
    "        print(f\"    Utilization: {measured_utilization:.1%} (theory: {theoretical['utilization']:.1%})\")\n",
    "        print(f\"    Success rate: {workload_results.success_rate:.1%}\")\n",
    "\n",
    "        print(\"Cooling Down\")\n",
    "        time.sleep(VALIDATION_COOLDOWN)\n",
    "\n",
    "    print(f\"\\nAll validation experiments completed\")\n",
    "    print(f\"Collected data for {len(validation_results)} load conditions\")\n",
    "    \n",
    "    return measured_metrics, theoretical_predictions, validation_results\n",
    "\n",
    "# Execute validation experiments\n",
    "measured_metrics, theoretical_predictions, validation_results = await run_validation_experiments(estimated_mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Results Analysis\n",
    "\n",
    "Comprehensive analysis and visualization of M/M/1 model validation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VALIDATION RESULTS ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_validation_results(theoretical_predictions, measured_metrics, estimated_mu):\n",
    "    \"\"\"\n",
    "    Analyze and display M/M/1 validation results.\n",
    "    \"\"\"\n",
    "    print(\"M/M/1 Model Validation Analysis\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    # Create comprehensive validation plots using imported function\n",
    "    plot_mm1_validation_analysis(theoretical_predictions, measured_metrics, estimated_mu)\n",
    "    \n",
    "    # Calculate statistical validation metrics using imported function\n",
    "    stats = calculate_validation_statistics(theoretical_predictions, measured_metrics)\n",
    "    \n",
    "    # Display statistical analysis\n",
    "    print(f\"\\nStatistical Validation Summary:\")\n",
    "    print(f\"=\" * 35)\n",
    "\n",
    "    if 'error' not in stats:\n",
    "        print(f\"Correlation coefficients:\")\n",
    "        print(f\"  Throughput: {stats['throughput_corr']:.3f}\")\n",
    "        print(f\"  Response Time: {stats['response_time_corr']:.3f}\")\n",
    "        print(f\"  Utilization: {stats['utilization_corr']:.3f}\")\n",
    "\n",
    "        print(f\"\\nMean Absolute Relative Errors:\")\n",
    "        print(f\"  Throughput: {stats['throughput_mae']:.1f}%\")\n",
    "        print(f\"  Response Time: {stats['response_time_mae']:.1f}%\")\n",
    "\n",
    "        print(f\"\\nOverall Model Assessment:\")\n",
    "        print(f\"  Model accuracy: {stats['assessment']}\")\n",
    "        print(f\"  Minimum correlation: {stats['min_correlation']:.3f}\")\n",
    "        print(f\"  Maximum error: {stats['max_error']:.1f}%\")\n",
    "    else:\n",
    "        print(stats['error'])\n",
    "    \n",
    "    # Additional response time source analysis\n",
    "    print(f\"\\nResponse Time Measurement Sources:\")\n",
    "    print(f\"=\" * 35)\n",
    "    \n",
    "    client_count = 0\n",
    "    server_count = 0\n",
    "    both_count = 0\n",
    "    \n",
    "    for lambda_rate, metrics in measured_metrics.items():\n",
    "        source = metrics['response_time_source']\n",
    "        rt_client = metrics['response_time_client']\n",
    "        rt_server = metrics['response_time_server']\n",
    "        \n",
    "        print(f\"λ={lambda_rate:.2f}: {source}\")\n",
    "        \n",
    "        if source == \"client-side\":\n",
    "            client_count += 1\n",
    "        elif source == \"server-side (Prometheus)\":\n",
    "            server_count += 1\n",
    "            \n",
    "        if rt_client is not None and rt_server is not None:\n",
    "            both_count += 1\n",
    "            diff_pct = abs(rt_client - rt_server) / rt_client * 100\n",
    "            print(f\"  Client: {rt_client:.3f}s, Server: {rt_server:.3f}s, Diff: {diff_pct:.1f}%\")\n",
    "    \n",
    "    print(f\"\\nResponse time measurement summary:\")\n",
    "    print(f\"  Client-side only: {client_count}\")\n",
    "    print(f\"  Server-side only: {server_count}\")\n",
    "    print(f\"  Both available: {both_count}\")\n",
    "\n",
    "# Execute validation analysis\n",
    "analyze_validation_results(theoretical_predictions, measured_metrics, estimated_mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Swarm-based Stress Test: Scaling mm1-server replicas\n",
    "\n",
    "This final section mirrors the validation but targets the Docker Swarm stack instead of the local docker-compose deployment. We keep the same Envoy entrypoint (`http://localhost:8084`) and Prometheus (`http://localhost:9090`).\n",
    "\n",
    "Prerequisites:\n",
    "- A Swarm stack deployed with `docker stack deploy -c docker-stack.yml spe`\n",
    "- Service name for the M/M/1 server in Swarm is `spe_mm1-server`\n",
    "\n",
    "What we do here:\n",
    "- Optionally scale `mm1-server` replicas (e.g., 1 → 2 → 3)\n",
    "- Generate an open-loop workload via Envoy (port 8084)\n",
    "- Collect Prometheus metrics in parallel\n",
    "- Compare measured throughput/response time across replica counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SWARM STRESS TEST: SCALE REPLICAS AND MEASURE (CLIENT-SIDE METRICS ONLY)\n",
    "# =============================================================================\n",
    "\n",
    "import subprocess\n",
    "import time\n",
    "from workload_generator import AsyncWorkloadGenerator\n",
    "\n",
    "SWARM_SERVICE_NAME = \"spe_mm1-server\"\n",
    "REPLICA_PLAN = [1,2]     # adjust as needed\n",
    "SWARM_SETTLE_SECONDS = 20    # wait after scaling for tasks to come up\n",
    "TEST_DURATION = 180          # per-replica test duration (seconds)\n",
    "INPUT_RATE=12\n",
    "\n",
    "async def run_swarm_stress_test():\n",
    "    print(\"Swarm Stress Test (Client-side metrics only)\")\n",
    "    print(\"=============================================\")\n",
    "\n",
    "    results_by_replicas = {}\n",
    "    prediction_by_replicas = {}\n",
    "\n",
    "    for replicas in REPLICA_PLAN:\n",
    "        print(f\"\\nScaling {SWARM_SERVICE_NAME} to {replicas} replicas ...\")\n",
    "        try:\n",
    "            subprocess.run(\n",
    "                [\"docker\", \"service\", \"scale\", f\"{SWARM_SERVICE_NAME}={replicas}\"],\n",
    "                check=True, capture_output=True, text=True\n",
    "            )\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(\"Failed to scale service:\", e.stderr)\n",
    "            raise\n",
    "\n",
    "        print(f\"Waiting {SWARM_SETTLE_SECONDS}s for Swarm to settle...\")\n",
    "        time.sleep(SWARM_SETTLE_SECONDS)\n",
    "\n",
    "        generator = AsyncWorkloadGenerator(target_url=\"http://localhost:8084\", timeout=300.0)\n",
    "\n",
    "        print(\"Generating workload...\")\n",
    "        \n",
    "        workload = await generator.generate_workload(\n",
    "            lambda_rate=INPUT_RATE, \n",
    "            duration=TEST_DURATION, \n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "        # Use ONLY client-side metrics from workload generator\n",
    "        measured = {\n",
    "            'replicas': replicas,\n",
    "            'workload_rate': workload.actual_rate,           # Client-side throughput\n",
    "            'success_rate': workload.success_rate,\n",
    "            'response_time_client': workload.average_response_time,  # Client-side response time\n",
    "            'total_requests': workload.total_requests,\n",
    "            'duration': workload.duration\n",
    "        }\n",
    "\n",
    "        results_by_replicas[replicas] = measured\n",
    "\n",
    "        print(f\"Completed replicas={replicas}: \"\n",
    "              f\"rate={measured['workload_rate']:.2f} req/s, \"\n",
    "              f\"rt={measured['response_time_client']:.3f}s, \"\n",
    "              f\"success={measured['success_rate']:.1%}\")\n",
    "\n",
    "    return results_by_replicas\n",
    "\n",
    "# Execute\n",
    "swarm_results = await run_swarm_stress_test()\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SWARM STRESS TEST SUMMARY (Client-side metrics)\")\n",
    "print(\"=\"*60)\n",
    "for r, m in swarm_results.items():\n",
    "    print(f\"Replicas: {r}\")\n",
    "    print(f\"  Throughput: {m['workload_rate']:.2f} req/s\")\n",
    "    print(f\"  Response Time: {m['response_time_client']:.3f}s\")\n",
    "    print(f\"  Success Rate: {m['success_rate']:.1%}\")\n",
    "    print(f\"  Total Requests: {m['total_requests']}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Swarm scaling results\n",
    "from swarm_plots import plot_swarm_scaling_results\n",
    "plot_swarm_scaling_results(swarm_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
